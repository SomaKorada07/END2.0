{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMAB1PsEh0OaVf/TwXUNPgb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomaKorada07/END2.0/blob/main/Session%205/Session_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwA6ZAP0gV57"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "ggimqGoi_BqW",
        "outputId": "ceb8e236-482e-455d-aef9-daf0a23ab7ad"
      },
      "source": [
        "df = pd.read_csv('/content/datasetSentences.txt', sep = '\\t', header = 0)\n",
        "df.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index                                           sentence\n",
              "0               1  The Rock is destined to be the 21st Century 's...\n",
              "1               2  The gorgeously elaborate continuation of `` Th...\n",
              "2               3                     Effective but too-tepid biopic\n",
              "3               4  If you sometimes like to go to the movies to h...\n",
              "4               5  Emerges as something rare , an issue movie tha..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiurY2gQK7iI",
        "outputId": "b956e522-c9b3-4a6c-e633-7fe275218f85"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sentence_index', 'sentence'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DelWJDZTRyA",
        "outputId": "efd6e70d-46b9-4d82-8313-aba3127486de"
      },
      "source": [
        "len(df['sentence_index']), len(df['sentence'])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11855, 11855)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "iIJeCQHsTZhQ",
        "outputId": "1e04decb-cd12-4e87-c809-fddaae9f8098"
      },
      "source": [
        "df_labels = pd.read_csv('/content/sentiment_labels.txt', sep = '|', header = 0)\n",
        "df_labels.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>sentiment values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.44444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.42708</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   phrase ids  sentiment values\n",
              "0           0           0.50000\n",
              "1           1           0.50000\n",
              "2           2           0.44444\n",
              "3           3           0.50000\n",
              "4           4           0.42708"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eB9MaMYURug",
        "outputId": "56136c98-780d-456e-8678-79aae8617972"
      },
      "source": [
        "len(df_labels)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "239232"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "YBZMpE52U4bO",
        "outputId": "b4e24363-eb1c-43ef-cc2b-5c5e841c4699"
      },
      "source": [
        "df_dict = pd.read_csv('/content/dictionary.txt', sep = '|', header = None)\n",
        "df_dict.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>! '</td>\n",
              "      <td>22935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>! ''</td>\n",
              "      <td>18235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>! Alas</td>\n",
              "      <td>179257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>! Brilliant</td>\n",
              "      <td>22936</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             0       1\n",
              "0            !       0\n",
              "1          ! '   22935\n",
              "2         ! ''   18235\n",
              "3       ! Alas  179257\n",
              "4  ! Brilliant   22936"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTAZoTo2VtPh",
        "outputId": "16d42f03-48ce-4c43-ab3f-e9099a7db117"
      },
      "source": [
        "df_dict.columns"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([0, 1], dtype='int64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDRsZFU793iO",
        "outputId": "b4414585-b7b2-4df0-e23e-720b1e40b726"
      },
      "source": [
        "type(df_dict[0]), type(df_dict[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(pandas.core.series.Series, pandas.core.series.Series)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5iFm-oU_NVc"
      },
      "source": [
        "df_dict.rename(columns = {0: 'phrase', 1: 'phrase ids'}, inplace = True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1HKRPz1-a5P"
      },
      "source": [
        "# pd.to_numeric(df_dict[1], downcast='integer')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "hjG4vzWE-2eu",
        "outputId": "41f58d21-25c2-46ef-f7b6-e929b6cc77b4"
      },
      "source": [
        "df_dict.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phrase</th>\n",
              "      <th>phrase ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>! '</td>\n",
              "      <td>22935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>! ''</td>\n",
              "      <td>18235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>! Alas</td>\n",
              "      <td>179257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>! Brilliant</td>\n",
              "      <td>22936</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        phrase  phrase ids\n",
              "0            !           0\n",
              "1          ! '       22935\n",
              "2         ! ''       18235\n",
              "3       ! Alas      179257\n",
              "4  ! Brilliant       22936"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2YC5_s2-4se"
      },
      "source": [
        "df_target = pd.merge(df_labels, df_dict, how = 'inner', left_on = 'phrase ids', right_on = 'phrase ids')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "MQcvoLqRAR9x",
        "outputId": "002fa957-227b-4e44-a591-514b0025f358"
      },
      "source": [
        "df_target.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.44444</td>\n",
              "      <td>' (</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>' ( the cockettes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.42708</td>\n",
              "      <td>' ( the cockettes )</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   phrase ids  sentiment values               phrase\n",
              "0           0           0.50000                    !\n",
              "1           1           0.50000                    '\n",
              "2           2           0.44444                  ' (\n",
              "3           3           0.50000    ' ( the cockettes\n",
              "4           4           0.42708  ' ( the cockettes )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIK9S5H7AY8c"
      },
      "source": [
        "common_sentences = df[df['sentence'].isin(df_target['phrase'])]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "5mMH9uladVEQ",
        "outputId": "b16f5690-8c1b-43bd-c5a9-c9567158b4c3"
      },
      "source": [
        "common_sentences"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11850</th>\n",
              "      <td>11851</td>\n",
              "      <td>A real snooze .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11851</th>\n",
              "      <td>11852</td>\n",
              "      <td>No surprises .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11852</th>\n",
              "      <td>11853</td>\n",
              "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11853</th>\n",
              "      <td>11854</td>\n",
              "      <td>Her fans walked out muttering words like `` ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11854</th>\n",
              "      <td>11855</td>\n",
              "      <td>In this case zero .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11286 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_index                                           sentence\n",
              "0                   1  The Rock is destined to be the 21st Century 's...\n",
              "1                   2  The gorgeously elaborate continuation of `` Th...\n",
              "2                   3                     Effective but too-tepid biopic\n",
              "3                   4  If you sometimes like to go to the movies to h...\n",
              "4                   5  Emerges as something rare , an issue movie tha...\n",
              "...               ...                                                ...\n",
              "11850           11851                                    A real snooze .\n",
              "11851           11852                                     No surprises .\n",
              "11852           11853  We 've seen the hippie-turned-yuppie plot befo...\n",
              "11853           11854  Her fans walked out muttering words like `` ho...\n",
              "11854           11855                                In this case zero .\n",
              "\n",
              "[11286 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URUeg8VRe-l_"
      },
      "source": [
        "def sentence_exists(input_sentence):\n",
        "    matching_phrase = df_target[df_target[\"phrase\"] == input_sentence][\"phrase ids\"].values\n",
        "    default_series = pd.Series({\"phrase ids\":-1000, \"sentiment values\": -1000}) ## For cases where we dont find a full sentence match\n",
        "    if(len(matching_phrase)) > 0:\n",
        "        phrase_id = np.int(matching_phrase[0])\n",
        "        sentiment_value = df_target[df_target[\"phrase ids\"] == matching_phrase[0]][\"sentiment values\"].values[0]\n",
        "        default_series = pd.Series({\"phrase ids\":phrase_id, \"sentiment values\": sentiment_value})\n",
        "\n",
        "    return default_series"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoLGHT5sdWet"
      },
      "source": [
        "common_sentences.loc[:, [\"phrase ids\", \"sentiment values\"]] =  common_sentences.loc[:,\"sentence\"].apply(sentence_exists)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "reRBbzyjghjN",
        "outputId": "ffe63130-1741-4299-ddf5-1c16e90069ee"
      },
      "source": [
        "common_sentences.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>sentiment values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>226166.0</td>\n",
              "      <td>0.69444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>226300.0</td>\n",
              "      <td>0.83333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>13995.0</td>\n",
              "      <td>0.51389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>14123.0</td>\n",
              "      <td>0.73611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>13999.0</td>\n",
              "      <td>0.86111</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index  ... sentiment values\n",
              "0               1  ...          0.69444\n",
              "1               2  ...          0.83333\n",
              "2               3  ...          0.51389\n",
              "3               4  ...          0.73611\n",
              "4               5  ...          0.86111\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAxDwEL7gAws",
        "outputId": "51185c56-124f-4cf6-833f-798d5eaafe71"
      },
      "source": [
        "pd.to_numeric(common_sentences['phrase ids'], downcast='integer')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        226166\n",
              "1        226300\n",
              "2         13995\n",
              "3         14123\n",
              "4         13999\n",
              "          ...  \n",
              "11850    222071\n",
              "11851    225165\n",
              "11852    226985\n",
              "11853    223632\n",
              "11854    224044\n",
              "Name: phrase ids, Length: 11286, dtype: int32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "2i9MdAXxhisV",
        "outputId": "5bb5a6af-45e5-410d-dcf9-4d12cceef69a"
      },
      "source": [
        "common_sentences.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>sentiment values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>226166.0</td>\n",
              "      <td>0.69444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>226300.0</td>\n",
              "      <td>0.83333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>13995.0</td>\n",
              "      <td>0.51389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>14123.0</td>\n",
              "      <td>0.73611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>13999.0</td>\n",
              "      <td>0.86111</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index  ... sentiment values\n",
              "0               1  ...          0.69444\n",
              "1               2  ...          0.83333\n",
              "2               3  ...          0.51389\n",
              "3               4  ...          0.73611\n",
              "4               5  ...          0.86111\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny757bpjhkN6",
        "outputId": "0ebc5f4e-3eaf-41ad-e6c1-5aefc3a6dd88"
      },
      "source": [
        "common_sentences.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11286, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8ddu7a2lgCk"
      },
      "source": [
        "# Training and test dataset\n",
        "After applying above techiniques we get a combined DataFrame that has the following fields:\n",
        "\n",
        "*   sentence_index : Original sentence_index in the datasetSentences.txt\n",
        "*   sentence: The actual sentence\n",
        "*   Phrase_Id: The Phrase_Id used for mapping/retrieving sentiment score\n",
        "*   Sentiment_Score: Actual Sentiment score for the sentence\n",
        "\n",
        "Now we split the DataFrame into Training and Test Dataframes. Note that we are not using the recommended train/ dev /test splits from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKMNKE38hwHa"
      },
      "source": [
        "train_df = common_sentences.sample(frac = 0.8) \n",
        "test_df = common_sentences[~common_sentences.sentence_index.isin(train_df.sentence_index)]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oQLB7lymZxj",
        "outputId": "90760118-ba48-4713-c531-1a216bb6a629"
      },
      "source": [
        "train_df.shape, test_df.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9029, 4), (2257, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQBYauo7meTH"
      },
      "source": [
        "train_df.to_csv(\"StanfordTrain.csv\", index=False, sep=\"|\")\n",
        "test_df.to_csv(\"StanfordTest.csv\", index=False, sep=\"|\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7uGPt4YqJno"
      },
      "source": [
        "# Data Augmentation Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSupdZC0qM9P"
      },
      "source": [
        "## **Random Swap**\n",
        "The random swap augmentation takes a sentence and then swaps words within it n times, with each iteration working on the previously swapped sentence. Here we sample two random numbers based on the length of the sentence, and then just keep swapping until we hit n."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqJzD5b3mvHn"
      },
      "source": [
        "def random_swap(sentence, n=5): \n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pao1Y2GzqazO"
      },
      "source": [
        "For more on this please go through this [paper](https://arxiv.org/pdf/1901.11196.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvTxBI8Pqfhe"
      },
      "source": [
        "##**Back Translation**\n",
        "\n",
        "Another popular approach for augmenting text datasets is back translation. This involves translating a sentence from our target language into one or more other languages and then translating all of them back to the original language. We can use the Python library googletrans for this purpose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVQTw0IIqzRQ",
        "outputId": "b3ecd43a-3b58-4109-8af8-f1c6cc06329e"
      },
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/3d/4e3a1609bf52f2f7b00436cc751eb977e27040665dde2bd57e7152989672/googletrans-3.1.0a0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.5)\n",
            "Collecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/50/606213e12fb49c5eb667df0936223dcaf461f94e215ea60244b2b1e9b039/hstspreload-2020.12.22-py3-none-any.whl (994kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 3.4MB/s \n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl\n",
            "Collecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.3MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.4MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.1MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-cp37-none-any.whl size=16368 sha256=1fa0dfa5c94fe69534d1353300028114e73d3c599556bc7a944ab26532603b35\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/7a/a0/aff3babbb775549ce6813cb8fa7ff3c0848c4dc62c20f8fdac\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hstspreload, rfc3986, sniffio, h11, hpack, hyperframe, h2, httpcore, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.12.22 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "HV1Hy9zuqocA",
        "outputId": "d0489a5c-8cd3-4001-95ab-458ee9df52e0"
      },
      "source": [
        "import random\n",
        "import googletrans\n",
        "#import googletrans.Translator as Translator\n",
        "\n",
        "translator = googletrans.Translator()\n",
        "sentence = ['The dog slept on the rug', 'ran lazily']\n",
        "\n",
        "available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "trans_lang = random.choice(available_langs) \n",
        "print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")\n",
        "\n",
        "trans_lang"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translating to urdu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ur'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sXuwBrqqo3E",
        "outputId": "e4cc8cb0-513a-421c-c54b-7100c91329c0"
      },
      "source": [
        "sentence = ['The dog slept on the rug']\n",
        "available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "trans_lang = random.choice(available_langs) \n",
        "print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translating to swedish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHWkiRkQrIne",
        "outputId": "1d08b235-4704-4629-ebe5-48bd89746342"
      },
      "source": [
        "translations = translator.translate(sentence, dest=trans_lang) \n",
        "t_text = [t.text for t in translations]\n",
        "print(t_text)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hunden sov på mattan']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjXm8dedrKIk",
        "outputId": "70ea9d6d-d847-4a77-ed91-44f52abe566b"
      },
      "source": [
        "translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n",
        "en_text = [t.text for t in translations_en_random]\n",
        "print(en_text)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The dog slept on the carpet']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmEFdQjVrN2T"
      },
      "source": [
        "translator = googletrans.Translator()\n",
        "\n",
        "def back_translate(sentence):\n",
        "    available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "    trans_lang = random.choice(available_langs) \n",
        "    translations = translator.translate(sentence, dest=trans_lang) \n",
        "    t_text = [t.text for t in translations]\n",
        "    translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n",
        "    en_text = [t.text for t in translations_en_random]\n",
        "    return en_text"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqVUb6o_rU0z",
        "outputId": "bbc8dcbf-c5ae-4481-df2b-916295699fd5"
      },
      "source": [
        "back_translate([common_sentences.iloc[0].sentence])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdZV_kkSrnRD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6e15BFUr8RV"
      },
      "source": [
        "\n",
        "## **Data Augmentation**\n",
        "We use the EDA techniques referred in this [code](https://github.com/jasonwei20/eda_nlp). Our parameters for augmentation are as follows:\n",
        "\n",
        "*   Num_aug = 5 i.e 5 augmented sentences\n",
        "*   alpha_rs=0.2 i.e 20% percent of words in each sentence to be swapped\n",
        "*   alpha_rd=0.2 i.e 20% percent of words in each sentence to be dropped\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKnX76gXvW0j"
      },
      "source": [
        "import random\n",
        "from random import shuffle\n",
        "random.seed(1)\n",
        "\n",
        "#stop words list\n",
        "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n",
        "\t\t\t'ours', 'ourselves', 'you', 'your', 'yours', \n",
        "\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', \n",
        "\t\t\t'himself', 'she', 'her', 'hers', 'herself', \n",
        "\t\t\t'it', 'its', 'itself', 'they', 'them', 'their', \n",
        "\t\t\t'theirs', 'themselves', 'what', 'which', 'who', \n",
        "\t\t\t'whom', 'this', 'that', 'these', 'those', 'am', \n",
        "\t\t\t'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
        "\t\t\t'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
        "\t\t\t'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
        "\t\t\t'because', 'as', 'until', 'while', 'of', 'at', \n",
        "\t\t\t'by', 'for', 'with', 'about', 'against', 'between',\n",
        "\t\t\t'into', 'through', 'during', 'before', 'after', \n",
        "\t\t\t'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
        "\t\t\t'out', 'on', 'off', 'over', 'under', 'again', \n",
        "\t\t\t'further', 'then', 'once', 'here', 'there', 'when', \n",
        "\t\t\t'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
        "\t\t\t'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
        "\t\t\t'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
        "\t\t\t'very', 's', 't', 'can', 'will', 'just', 'don', \n",
        "\t\t\t'should', 'now', '']\n",
        "\n",
        "#cleaning up text\n",
        "import re\n",
        "def get_only_chars(line):\n",
        "\n",
        "    clean_line = \"\"\n",
        "\n",
        "    line = line.replace(\"’\", \"\")\n",
        "    line = line.replace(\"'\", \"\")\n",
        "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
        "    line = line.replace(\"\\t\", \" \")\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    line = line.lower()\n",
        "\n",
        "    for char in line:\n",
        "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
        "            clean_line += char\n",
        "        else:\n",
        "            clean_line += ' '\n",
        "\n",
        "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
        "    if clean_line[0] == ' ':\n",
        "        clean_line = clean_line[1:]\n",
        "    return clean_line\n",
        "\n",
        "########################################################################\n",
        "# Synonym replacement\n",
        "# Replace n words in the sentence with synonyms from wordnet\n",
        "########################################################################\n",
        "\n",
        "#for the first time you use wordnet\n",
        "#import nltk\n",
        "#nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet \n",
        "\n",
        "def synonym_replacement(words, n):\n",
        "\tnew_words = words.copy()\n",
        "\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n",
        "\trandom.shuffle(random_word_list)\n",
        "\tnum_replaced = 0\n",
        "\tfor random_word in random_word_list:\n",
        "\t\tsynonyms = get_synonyms(random_word)\n",
        "\t\tif len(synonyms) >= 1:\n",
        "\t\t\tsynonym = random.choice(list(synonyms))\n",
        "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
        "\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n",
        "\t\t\tnum_replaced += 1\n",
        "\t\tif num_replaced >= n: #only replace up to n words\n",
        "\t\t\tbreak\n",
        "\n",
        "\t#this is stupid but we need it, trust me\n",
        "\tsentence = ' '.join(new_words)\n",
        "\tnew_words = sentence.split(' ')\n",
        "\n",
        "\treturn new_words\n",
        "\n",
        "def get_synonyms(word):\n",
        "\tsynonyms = set()\n",
        "\tfor syn in wordnet.synsets(word): \n",
        "\t\tfor l in syn.lemmas(): \n",
        "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
        "\t\t\tsynonyms.add(synonym) \n",
        "\tif word in synonyms:\n",
        "\t\tsynonyms.remove(word)\n",
        "\treturn list(synonyms)\n",
        "\n",
        "########################################################################\n",
        "# Random deletion\n",
        "# Randomly delete words from the sentence with probability p\n",
        "########################################################################\n",
        "\n",
        "def random_deletion(words, p):\n",
        "\n",
        "\t#obviously, if there's only one word, don't delete it\n",
        "\tif len(words) == 1:\n",
        "\t\treturn words\n",
        "\n",
        "\t#randomly delete words with probability p\n",
        "\tnew_words = []\n",
        "\tfor word in words:\n",
        "\t\tr = random.uniform(0, 1)\n",
        "\t\tif r > p:\n",
        "\t\t\tnew_words.append(word)\n",
        "\n",
        "\t#if you end up deleting all words, just return a random word\n",
        "\tif len(new_words) == 0:\n",
        "\t\trand_int = random.randint(0, len(words)-1)\n",
        "\t\treturn [words[rand_int]]\n",
        "\n",
        "\treturn new_words\n",
        "\n",
        "########################################################################\n",
        "# Random swap\n",
        "# Randomly swap two words in the sentence n times\n",
        "########################################################################\n",
        "\n",
        "def random_swap(words, n):\n",
        "\tnew_words = words.copy()\n",
        "\tfor _ in range(n):\n",
        "\t\tnew_words = swap_word(new_words)\n",
        "\treturn new_words\n",
        "\n",
        "def swap_word(new_words):\n",
        "\trandom_idx_1 = random.randint(0, len(new_words)-1)\n",
        "\trandom_idx_2 = random_idx_1\n",
        "\tcounter = 0\n",
        "\twhile random_idx_2 == random_idx_1:\n",
        "\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n",
        "\t\tcounter += 1\n",
        "\t\tif counter > 3:\n",
        "\t\t\treturn new_words\n",
        "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
        "\treturn new_words\n",
        "\n",
        "########################################################################\n",
        "# Random insertion\n",
        "# Randomly insert n words into the sentence\n",
        "########################################################################\n",
        "\n",
        "def random_insertion(words, n):\n",
        "\tnew_words = words.copy()\n",
        "\tfor _ in range(n):\n",
        "\t\tadd_word(new_words)\n",
        "\treturn new_words\n",
        "\n",
        "def add_word(new_words):\n",
        "\tsynonyms = []\n",
        "\tcounter = 0\n",
        "\twhile len(synonyms) < 1:\n",
        "\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n",
        "\t\tsynonyms = get_synonyms(random_word)\n",
        "\t\tcounter += 1\n",
        "\t\tif counter >= 10:\n",
        "\t\t\treturn\n",
        "\trandom_synonym = synonyms[0]\n",
        "\trandom_idx = random.randint(0, len(new_words)-1)\n",
        "\tnew_words.insert(random_idx, random_synonym)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLk4iZGYvUQN"
      },
      "source": [
        "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
        "\t\n",
        "\tsentence = get_only_chars(sentence)\n",
        "\twords = sentence.split(' ')\n",
        "\twords = [word for word in words if word is not '']\n",
        "\tnum_words = len(words)\n",
        "\t\n",
        "\taugmented_sentences = []\n",
        "\tnum_new_per_technique = int(num_aug/4)+1\n",
        "\n",
        "\t#sr\n",
        "\tif (alpha_sr > 0):\n",
        "\t\tn_sr = max(1, int(alpha_sr*num_words))\n",
        "\t\tfor _ in range(num_new_per_technique):\n",
        "\t\t\ta_words = synonym_replacement(words, n_sr)\n",
        "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "\t#ri\n",
        "\tif (alpha_ri > 0):\n",
        "\t\tn_ri = max(1, int(alpha_ri*num_words))\n",
        "\t\tfor _ in range(num_new_per_technique):\n",
        "\t\t\ta_words = random_insertion(words, n_ri)\n",
        "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "\t#rs\n",
        "\tif (alpha_rs > 0):\n",
        "\t\tn_rs = max(1, int(alpha_rs*num_words))\n",
        "\t\tfor _ in range(num_new_per_technique):\n",
        "\t\t\ta_words = random_swap(words, n_rs)\n",
        "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "\t#rd\n",
        "\tif (p_rd > 0):\n",
        "\t\tfor _ in range(num_new_per_technique):\n",
        "\t\t\ta_words = random_deletion(words, p_rd)\n",
        "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "\taugmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
        "\tshuffle(augmented_sentences)\n",
        "\n",
        "\t#trim so that we have the desired number of augmented sentences\n",
        "\tif num_aug >= 1:\n",
        "\t\taugmented_sentences = augmented_sentences[:num_aug]\n",
        "\telse:\n",
        "\t\tkeep_prob = num_aug / len(augmented_sentences)\n",
        "\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
        "\n",
        "\t#append the original sentence\n",
        "\taugmented_sentences.append(sentence)\n",
        "\n",
        "\treturn augmented_sentences"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "kgMo4RZUxCPP",
        "outputId": "e7498e1c-d1be-430f-a65c-3211edd825d3"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>sentiment values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6681</th>\n",
              "      <td>6682</td>\n",
              "      <td>It 's got the brawn , but not the brains .</td>\n",
              "      <td>106881.0</td>\n",
              "      <td>0.31944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8685</th>\n",
              "      <td>8686</td>\n",
              "      <td>Just one problem : Fish out of water usually d...</td>\n",
              "      <td>146972.0</td>\n",
              "      <td>0.44444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7541</th>\n",
              "      <td>7542</td>\n",
              "      <td>A very long movie , dull in stretches , with e...</td>\n",
              "      <td>143420.0</td>\n",
              "      <td>0.19444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3535</th>\n",
              "      <td>3536</td>\n",
              "      <td>Director Juan Jose Campanella could have turne...</td>\n",
              "      <td>223149.0</td>\n",
              "      <td>0.93056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1566</th>\n",
              "      <td>1567</td>\n",
              "      <td>Writer-director 's Mehta 's effort has tons of...</td>\n",
              "      <td>47695.0</td>\n",
              "      <td>0.83333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      sentence_index  ... sentiment values\n",
              "6681            6682  ...          0.31944\n",
              "8685            8686  ...          0.44444\n",
              "7541            7542  ...          0.19444\n",
              "3535            3536  ...          0.93056\n",
              "1566            1567  ...          0.83333\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mca6Z06sTEi"
      },
      "source": [
        "# from eda import *\n",
        "simpler_series_df = train_df.copy()\n",
        "empty_series = { col:[] for col in train_df.columns.values}\n",
        "my_new_df = pd.DataFrame(data=empty_series)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hajx-ZU3sidO"
      },
      "source": [
        "simpler_series_df = train_df"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "eSYPGr7SxIO7",
        "outputId": "8e4067a1-c383-47e5-947c-5cadcd4ed283"
      },
      "source": [
        "simpler_series_df.head()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>sentiment values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6681</th>\n",
              "      <td>6682</td>\n",
              "      <td>It 's got the brawn , but not the brains .</td>\n",
              "      <td>106881.0</td>\n",
              "      <td>0.31944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8685</th>\n",
              "      <td>8686</td>\n",
              "      <td>Just one problem : Fish out of water usually d...</td>\n",
              "      <td>146972.0</td>\n",
              "      <td>0.44444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7541</th>\n",
              "      <td>7542</td>\n",
              "      <td>A very long movie , dull in stretches , with e...</td>\n",
              "      <td>143420.0</td>\n",
              "      <td>0.19444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3535</th>\n",
              "      <td>3536</td>\n",
              "      <td>Director Juan Jose Campanella could have turne...</td>\n",
              "      <td>223149.0</td>\n",
              "      <td>0.93056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1566</th>\n",
              "      <td>1567</td>\n",
              "      <td>Writer-director 's Mehta 's effort has tons of...</td>\n",
              "      <td>47695.0</td>\n",
              "      <td>0.83333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      sentence_index  ... sentiment values\n",
              "6681            6682  ...          0.31944\n",
              "8685            8686  ...          0.44444\n",
              "7541            7542  ...          0.19444\n",
              "3535            3536  ...          0.93056\n",
              "1566            1567  ...          0.83333\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4LNlCnKuTW0"
      },
      "source": [
        "max_sentence_ids = simpler_series_df.sentence_index.max()\n",
        "alpha_sr = 0.0\n",
        "alpha_ri=0.0\n",
        "alpha_rs=0.2\n",
        "alpha_rd=0.2\n",
        "num_aug=5\n",
        "\n",
        "for idx in simpler_series_df.itertuples():\n",
        "    # print(idx)\n",
        "    sentence = idx.sentence\n",
        "    aug_sentences = eda(sentence, alpha_sr=alpha_sr, alpha_ri=alpha_ri, alpha_rs=alpha_rs, p_rd=alpha_rd, num_aug=num_aug)\n",
        "    for aug_sentence in aug_sentences:\n",
        "        max_sentence_ids += 1\n",
        "        my_new_df.loc[len(my_new_df)] = [max_sentence_ids,aug_sentence, np.int(idx._3), idx._4 ]"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "-OZnjPrNxqrs",
        "outputId": "297461de-eaaf-4a47-dab9-f427785313bb"
      },
      "source": [
        "my_new_df"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>sentiment values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11855.0</td>\n",
              "      <td>it s got brains brawn but not the the</td>\n",
              "      <td>106881.0</td>\n",
              "      <td>0.31944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11856.0</td>\n",
              "      <td>it s got the but not the brains</td>\n",
              "      <td>106881.0</td>\n",
              "      <td>0.31944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11857.0</td>\n",
              "      <td>it s got but not the</td>\n",
              "      <td>106881.0</td>\n",
              "      <td>0.31944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11858.0</td>\n",
              "      <td>it s got the brawn the not but brains</td>\n",
              "      <td>106881.0</td>\n",
              "      <td>0.31944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11859.0</td>\n",
              "      <td>it s got the brawn but not the brains</td>\n",
              "      <td>106881.0</td>\n",
              "      <td>0.31944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48915</th>\n",
              "      <td>56995.0</td>\n",
              "      <td>there nothing exactly wrong here but there s n...</td>\n",
              "      <td>188845.0</td>\n",
              "      <td>0.33333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48916</th>\n",
              "      <td>56996.0</td>\n",
              "      <td>there s nothing exactly wrong here that s near...</td>\n",
              "      <td>188845.0</td>\n",
              "      <td>0.33333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48917</th>\n",
              "      <td>56997.0</td>\n",
              "      <td>there s nothing nearly wrong that but there s ...</td>\n",
              "      <td>188845.0</td>\n",
              "      <td>0.33333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48918</th>\n",
              "      <td>56998.0</td>\n",
              "      <td>there s nothing exactly wrong but there s not ...</td>\n",
              "      <td>188845.0</td>\n",
              "      <td>0.33333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48919</th>\n",
              "      <td>56999.0</td>\n",
              "      <td>there s nothing exactly wrong here but there s...</td>\n",
              "      <td>188845.0</td>\n",
              "      <td>0.33333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>48920 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_index  ... sentiment values\n",
              "0             11855.0  ...          0.31944\n",
              "1             11856.0  ...          0.31944\n",
              "2             11857.0  ...          0.31944\n",
              "3             11858.0  ...          0.31944\n",
              "4             11859.0  ...          0.31944\n",
              "...               ...  ...              ...\n",
              "48915         56995.0  ...          0.33333\n",
              "48916         56996.0  ...          0.33333\n",
              "48917         56997.0  ...          0.33333\n",
              "48918         56998.0  ...          0.33333\n",
              "48919         56999.0  ...          0.33333\n",
              "\n",
              "[48920 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqFEflFHuqDy"
      },
      "source": [
        "my_new_df = my_new_df.sample(frac = 1)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqBzNONWxtqK"
      },
      "source": [
        "my_new_df.to_csv(\"StanfordAugmented.csv\", index = False, sep = \"|\")"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpWRHzOTxv1P"
      },
      "source": [
        "my_new_df = my_new_df.append(train_df, ignore_index = True)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpYVYBg9x2C_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVhp-7plyDhO"
      },
      "source": [
        "\n",
        "## **Augmenting with Back_Translate**\n",
        "In this method we make a translate from source English sentence to a sentence in random language and then back into English. We create a dataframe of around 4000 samples and append it back to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b5jlHtJyFd-"
      },
      "source": [
        "translator = googletrans.Translator()\n",
        "\n",
        "def back_translate(sentence):\n",
        "    available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "    trans_lang = random.choice(available_langs) \n",
        "    translations = translator.translate(sentence, dest=trans_lang) \n",
        "    t_text = [t.text for t in translations]\n",
        "    translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n",
        "    en_text = [t.text for t in translations_en_random]\n",
        "    return en_text"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2auxXF5yKLu"
      },
      "source": [
        "max_sentence_ids=simpler_series_df[\"sentence_index\"].max()\n",
        "for idx in my_new_df[2000:6000].itertuples():\n",
        "    sentence = idx.sentence\n",
        "    aug_sentence = back_translate([sentence])[0]\n",
        "    max_sentence_ids += 1\n",
        "    simpler_series_df.loc[len(simpler_series_df)] = [max_sentence_ids,aug_sentence, np.int(idx._3), idx._4]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjPMvPE5zT_O"
      },
      "source": [
        "simpler_series_df.to_csv(\"StanfordTranslate.csv\", index = False, sep = \"|\")"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbXOj7uvzirj"
      },
      "source": [
        "my_new_df = my_new_df.append(train_df, ignore_index = True)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfUPXdYHzlwu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wN4W9Q7zoGD"
      },
      "source": [
        "## **Range expansion**\n",
        "The sentiment scores are floats between [0,1] so we convert this to equivalent integers between 0 and 24(i.e 25 different classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZnU3NYWztsO"
      },
      "source": [
        "my_new_df[\"label\"] = 0\n",
        "\n",
        "def quantize_predictions(score_val):\n",
        "    return np.floor(score_val * 24)\n",
        "\n",
        "my_new_df.loc[:,\"label\"] = my_new_df.loc[:,\"sentiment values\"].apply(quantize_predictions)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z5z-yk_z8e-"
      },
      "source": [
        "my_new_df[\"sentence_index\"] = my_new_df[\"sentence_index\"].astype('int')\n",
        "my_new_df[\"Phrase_Id\"] = my_new_df[\"phrase ids\"].astype('int')\n",
        "my_new_df[\"label\"] = my_new_df[\"label\"].astype('int')"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "kgwaRagM0Lqk",
        "outputId": "d9b24285-837a-46e5-de85-2eb634b3ddc9"
      },
      "source": [
        "my_new_df"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>label</th>\n",
              "      <th>Phrase_Id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>42477</td>\n",
              "      <td>an enthralling aesthetic experience one that s...</td>\n",
              "      <td>44483.0</td>\n",
              "      <td>0.819440</td>\n",
              "      <td>19</td>\n",
              "      <td>44483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>45093</td>\n",
              "      <td>the entire film big excuse to one lewd after a...</td>\n",
              "      <td>226240.0</td>\n",
              "      <td>0.069444</td>\n",
              "      <td>1</td>\n",
              "      <td>226240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18650</td>\n",
              "      <td>so only than undercover time also funny if mor...</td>\n",
              "      <td>26229.0</td>\n",
              "      <td>0.819440</td>\n",
              "      <td>19</td>\n",
              "      <td>26229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>35622</td>\n",
              "      <td>holofcener rejects solutions to life s messine...</td>\n",
              "      <td>66043.0</td>\n",
              "      <td>0.847220</td>\n",
              "      <td>20</td>\n",
              "      <td>66043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12393</td>\n",
              "      <td>nicole holofcener the lovely and amazing from ...</td>\n",
              "      <td>107994.0</td>\n",
              "      <td>0.847220</td>\n",
              "      <td>20</td>\n",
              "      <td>107994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66974</th>\n",
              "      <td>11261</td>\n",
              "      <td>Certainly not a good movie , but it was n't ho...</td>\n",
              "      <td>183042.0</td>\n",
              "      <td>0.305560</td>\n",
              "      <td>7</td>\n",
              "      <td>183042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66975</th>\n",
              "      <td>1234</td>\n",
              "      <td>A warm but realistic meditation on friendship ...</td>\n",
              "      <td>24505.0</td>\n",
              "      <td>0.833330</td>\n",
              "      <td>19</td>\n",
              "      <td>24505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66976</th>\n",
              "      <td>5968</td>\n",
              "      <td>With a tone as variable as the cinematography ...</td>\n",
              "      <td>111193.0</td>\n",
              "      <td>0.208330</td>\n",
              "      <td>4</td>\n",
              "      <td>111193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66977</th>\n",
              "      <td>9206</td>\n",
              "      <td>There 's nothing exactly wrong here , but ther...</td>\n",
              "      <td>188845.0</td>\n",
              "      <td>0.333330</td>\n",
              "      <td>7</td>\n",
              "      <td>188845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66978</th>\n",
              "      <td>11855</td>\n",
              "      <td>not, and thus a feature of the creature, but s...</td>\n",
              "      <td>186430.0</td>\n",
              "      <td>0.791670</td>\n",
              "      <td>19</td>\n",
              "      <td>186430</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>66979 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_index  ... Phrase_Id\n",
              "0               42477  ...     44483\n",
              "1               45093  ...    226240\n",
              "2               18650  ...     26229\n",
              "3               35622  ...     66043\n",
              "4               12393  ...    107994\n",
              "...               ...  ...       ...\n",
              "66974           11261  ...    183042\n",
              "66975            1234  ...     24505\n",
              "66976            5968  ...    111193\n",
              "66977            9206  ...    188845\n",
              "66978           11855  ...    186430\n",
              "\n",
              "[66979 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw1MNfvk0oh4"
      },
      "source": [
        "my_new_df.to_csv(\"StanFullTrain.csv\", index = False, sep = \"|\")"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUgfDZuM0pOi"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F0WxI240xLt",
        "outputId": "2fda2e6e-5c7c-4b8c-c6d9-4c4e8a1609ac"
      },
      "source": [
        "# Import Library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext.legacy import data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f53550920b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sxPzjmb0996"
      },
      "source": [
        "my_new_df = pd.read_csv(\"StanFullTrain.csv\", sep=\"|\")"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "etsp9N790_qq",
        "outputId": "395b5e3d-c9e9-49eb-b847-b23c951154c5"
      },
      "source": [
        "num_bins = 25\n",
        "\n",
        "colors = ['green'] \n",
        "plt.hist(my_new_df.label, density = False, bins = 25, histtype = 'bar', color = colors, label = colors)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 423., 1453., 1239., 2075., 4497., 2671., 4474., 4892., 1962.,\n",
              "        3089., 3674., 1370., 3052., 3207., 1909., 2751., 4822., 2907.,\n",
              "        4556., 5590., 1804., 2827., 1285.,  277.,  173.]),\n",
              " array([ 0.  ,  0.96,  1.92,  2.88,  3.84,  4.8 ,  5.76,  6.72,  7.68,\n",
              "         8.64,  9.6 , 10.56, 11.52, 12.48, 13.44, 14.4 , 15.36, 16.32,\n",
              "        17.28, 18.24, 19.2 , 20.16, 21.12, 22.08, 23.04, 24.  ]),\n",
              " <a list of 25 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBwnCl821V9q"
      },
      "source": [
        "Sentence = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
        "Label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALVglAZ71Y-7"
      },
      "source": [
        "fields = [('sentence', Sentence),('label',Label)]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4pUzcwQ1bRi"
      },
      "source": [
        "example = [data.Example.fromlist([my_new_df.sentence[i],my_new_df.label[i]], fields) for i in range(my_new_df.shape[0])] \n",
        "\n",
        "stanTreeDataset = data.Dataset(example, fields)\n",
        "(train, valid) = stanTreeDataset.split(split_ratio=[0.85, 0.15], random_state=random.seed(SEED))\n",
        "Sentence.build_vocab(train)\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwQMYfPF1fGB"
      },
      "source": [
        "(len(train), len(valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LddtK8AX1h7-"
      },
      "source": [
        "print('Size of input vocab : ', len(Sentence.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Sentence.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbQ2f7Al1kN1"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBz9dcf81mSR"
      },
      "source": [
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid), batch_size = 32, \n",
        "                                                            sort_key = lambda x: len(x.sentence),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EOcbV8L1ouJ"
      },
      "source": [
        "next(iter(train_iterator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YspsraVl1reJ"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Sentence.vocab.stoi, tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUdVyUS21xc5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNu49KiUAFuL"
      },
      "source": [
        "## **Model Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7YOPNmaAJRo"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAq5sCSvAKd1"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Sentence.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 100\n",
        "num_output_nodes = len(my_new_df.label.unique())\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgG8C_bFAMkl"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdqBf8dDAN-l"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FopJSB4TAPwU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MOMojDwASLh"
      },
      "source": [
        "## Model Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmmzRJBKAWAJ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        sentence, sentence_lengths = batch.sentence\n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(sentence, sentence_lengths).squeeze()  \n",
        "        #print(predictions)\n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.label)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.label)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM5cNegKAWyF"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            sentence, sentence_lengths = batch.sentence\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(sentence, sentence_lengths).squeeze()\n",
        "            #print(predictions)\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6xxtUijAjEo"
      },
      "source": [
        "N_EPOCHS = 100\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    #train_loss = train(model, train_iterator, optimizer, criterion)\n",
        "\n",
        "\n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    #valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tEpoch: {epoch} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9JkqMp9Ak70"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    #categories = {0: \"Negative\", 1:\"Positive\", 2:\"Neutral\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    #return categories[pred.item()]\n",
        "    return pred.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r3x1unLAraU"
      },
      "source": [
        "classify_tweet(\"A valid explanation for why Trump won't let women on the golf course.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLhjQ4hOAs6k"
      },
      "source": [
        "classify_tweet(\"The movie had zilch character, nada screenplay, zero action scenes, almost negligible thougts\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Df3b5bAuD0"
      },
      "source": [
        "classify_tweet(\"5 minutes into the movie, you feel you want to runaway but eventually the movie catches on and does a great job\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV4HFfUZAva0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}